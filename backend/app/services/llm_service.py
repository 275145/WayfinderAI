import os
from openai import OpenAI
from typing import Literal
from ..config import settings
from ..observability.logger import default_logger as logger
from typing import Literal, Optional, Iterator
Provider = Literal["openai", "zhipu", "modelscope", "ollama", "vllm", "custom"]

class LLMService:
    """
    ä¸€ä¸ªæ™ºèƒ½çš„ã€æ”¯æŒå¤šæœåŠ¡å•†çš„LLMæœåŠ¡ã€‚
    å®ƒèƒ½æ ¹æ®ç¯å¢ƒå˜é‡è‡ªåŠ¨æ£€æµ‹å¹¶é…ç½®LLMæä¾›å•†ã€‚
    """
    def __init__(self,
                 temperature: float = 0.7,
                 max_tokens: int = 4096,
                 timeout: Optional[int] = None,
                 **kwargs):
        """
        åˆå§‹åŒ–æœåŠ¡ï¼Œè‡ªåŠ¨æ£€æµ‹å¹¶é…ç½®å®¢æˆ·ç«¯ã€‚
        """
        self.provider: Provider = "custom"
        self.api_key: str | None = None
        self.base_url: str | None = None
        self.model: str | None = None


        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout or int(os.getenv("LLM_TIMEOUT", "60"))
        self.kwargs = kwargs
        # æ ¸å¿ƒé€»è¾‘ï¼šè‡ªåŠ¨æ£€æµ‹å’Œè§£æå‡­è¯
        self._auto_detect_provider()
        self._resolve_credentials()

        if not self.api_key:
            logger.warning("LLM API Key æœªé…ç½®ï¼ŒLLMæœåŠ¡å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œã€‚")

        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼ˆå…¼å®¹å¤šç§æœåŠ¡ï¼‰
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            timeout=settings.LLM_TIMEOUT
        )
        logger.info(f"LLMæœåŠ¡åˆå§‹åŒ–å®Œæˆã€‚Provider: {self.provider}, Model: {self.model}, Base URL: {self.base_url}")

    def _auto_detect_provider(self):
        """
        æ ¹æ®ç¯å¢ƒå˜é‡è‡ªåŠ¨æ¨æ–­LLMæœåŠ¡å•†ã€‚
        """
        # æœ€é«˜ä¼˜å…ˆçº§ï¼šæ ¹æ® base_url è¿›è¡Œåˆ¤æ–­
        base_url = settings.LLM_BASE_URL
        if base_url:
            if "api.openai.com" in base_url: self.provider = "openai"; return
            if "open.bigmodel.cn" in base_url: self.provider = "zhipu"; return
            if "api-inference.modelscope.cn" in base_url: self.provider = "modelscope"; return
            if ":11434" in base_url: self.provider = "ollama"; return
            if ":8000" in base_url: self.provider = "vllm"; return # å‡è®¾vllmåœ¨8000ç«¯å£

        # æ¬¡é«˜ä¼˜å…ˆçº§ï¼šæ£€æŸ¥ç‰¹å®šæœåŠ¡å•†çš„ç¯å¢ƒå˜é‡
        if settings.OPENAI_API_KEY: self.provider = "openai"; return
        if settings.ZHIPU_API_KEY: self.provider = "zhipu"; return
        if settings.MODELSCOPE_API_KEY: self.provider = "modelscope"; return

        # è¾…åŠ©åˆ¤æ–­ï¼šåˆ†æé€šç”¨APIå¯†é’¥æ ¼å¼ (ç¤ºä¾‹)
        api_key = settings.LLM_API_KEY
        if api_key:
            if api_key.startswith("sk-"): self.provider = "openai"; return
            # Zhipuå’ŒModelScopeçš„keyæ ¼å¼ä¸å¦‚æ­¤ç‹¬ç‰¹ï¼Œæ­¤å¤„çœç•¥ä»¥é¿å…è¯¯åˆ¤

        logger.info("æœªèƒ½è‡ªåŠ¨æ£€æµ‹åˆ°ç‰¹å®šçš„LLM Providerï¼Œå°†ä½¿ç”¨ 'custom' æ¨¡å¼ã€‚")

    def _resolve_credentials(self):
        """
        æ ¹æ®æ£€æµ‹åˆ°çš„æœåŠ¡å•†ï¼Œè§£æå¹¶è®¾ç½®æœ€ç»ˆçš„api_key, base_url, å’Œ modelã€‚
        """
        # 1. è§£æAPI Key
        provider_key = getattr(settings, f"{self.provider.upper()}_API_KEY", None)
        self.api_key = provider_key or settings.LLM_API_KEY

        # 2. è§£æBase URL
        if self.provider == "openai" and not settings.LLM_BASE_URL:
            self.base_url = "https://api.openai.com/v1"
        elif self.provider == "zhipu" and not settings.LLM_BASE_URL:
            self.base_url = "https://open.bigmodel.cn/api/paas/v4/"
        elif self.provider == "modelscope" and not settings.LLM_BASE_URL:
            self.base_url = "https://api-inference.modelscope.cn/v1"
        else:
            self.base_url = settings.LLM_BASE_URL

        # 3. è§£ææ¨¡å‹ID
        self.model = settings.LLM_MODEL_ID or "gpt-4-turbo" # æä¾›ä¸€ä¸ªé»˜è®¤å€¼

    def generate_json_plan(self, prompt: str) -> str:
        """
        è°ƒç”¨LLMç”ŸæˆJSONæ ¼å¼çš„è¡Œç¨‹è®¡åˆ’ã€‚æ­¤æ–¹æ³•ä¿æŒæ¥å£ä¸å˜ã€‚
        """
        if not self.api_key:
            logger.error("LLM API Key æœªé…ç½®ï¼Œæ— æ³•å‘èµ·è¯·æ±‚ã€‚")
            return ""

        logger.info(f"å‘LLM ({self.provider}) å‘èµ·è¡Œç¨‹è§„åˆ’è¯·æ±‚...")
        try:
            response = self.client.chat.completions.create(
                model=self.model, # ä½¿ç”¨è§£æåçš„æ¨¡å‹
                messages=[
                    {"role": "system", "content": "You are a helpful travel planner. You will output a travel plan in JSON format based on user requirements."},
                    {"role": "user", "content": prompt}
                ],
                response_format={"type": "json_object"}
            )
            json_output = response.choices[0].message.content
            logger.info("æˆåŠŸä»LLMè·å–åˆ°è¡Œç¨‹è§„åˆ’JSONæ•°æ®ã€‚")
            return json_output or ""
        except Exception as e:
            logger.error(f"è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            return ""
    def think(self, messages: list[dict[str, str]], temperature: Optional[float] = None) -> Iterator[str]:
        """
        è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ€è€ƒï¼Œå¹¶è¿”å›æµå¼å“åº”ã€‚
        è¿™æ˜¯ä¸»è¦çš„è°ƒç”¨æ–¹æ³•ï¼Œé»˜è®¤ä½¿ç”¨æµå¼å“åº”ä»¥è·å¾—æ›´å¥½çš„ç”¨æˆ·ä½“éªŒã€‚

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°ï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨åˆå§‹åŒ–æ—¶çš„å€¼

        Yields:
            str: æµå¼å“åº”çš„æ–‡æœ¬ç‰‡æ®µ
        """
        print(f"ğŸ§  æ­£åœ¨è°ƒç”¨ {self.model} æ¨¡å‹...")
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature if temperature is not None else self.temperature,
                max_tokens=self.max_tokens,
                stream=True,
            )

            # å¤„ç†æµå¼å“åº”
            print("âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:")
            for chunk in response:
                content = chunk.choices[0].delta.content or ""
                if content:
                    print(content, end="", flush=True)
                    yield content
            print()  # åœ¨æµå¼è¾“å‡ºç»“æŸåæ¢è¡Œ

        except Exception as e:
            print(f"âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")

    def invoke(self, messages: list[dict[str, str]], **kwargs) -> str:
        """
        éæµå¼è°ƒç”¨LLMï¼Œè¿”å›å®Œæ•´å“åº”ã€‚
        é€‚ç”¨äºä¸éœ€è¦æµå¼è¾“å‡ºçš„åœºæ™¯ã€‚
        """
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=kwargs.get('temperature', self.temperature),
                max_tokens=kwargs.get('max_tokens', self.max_tokens),
                **{k: v for k, v in kwargs.items() if k not in ['temperature', 'max_tokens']}
            )
            return response.choices[0].message.content
        except Exception as e:
            raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")

    def stream_invoke(self, messages: list[dict[str, str]], **kwargs) -> Iterator[str]:
        """
        æµå¼è°ƒç”¨LLMçš„åˆ«åæ–¹æ³•ï¼Œä¸thinkæ–¹æ³•åŠŸèƒ½ç›¸åŒã€‚
        ä¿æŒå‘åå…¼å®¹æ€§ã€‚
        """
        temperature = kwargs.get('temperature')
        yield from self.think(messages, temperature)
# åˆ›å»ºä¸€ä¸ªæœåŠ¡å®ä¾‹ï¼ŒFastAPIçš„ä¾èµ–æ³¨å…¥ç³»ç»Ÿå°†ä½¿ç”¨å®ƒ
llm_service = LLMService()